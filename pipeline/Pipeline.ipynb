{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime Prediction using Tweets and KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import functools\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "from utils.consts import START_DATE, END_DATE, KDE_BANDWITH, LDA_PARAMS, LDA_TOPICS, \\\n",
    "                         CSV_DATE_FORMART\n",
    "\n",
    "\n",
    "from utils.geo import latlng2grid_cords_chicago, \\\n",
    "                      latlng2LDA_topics_chicago, \\\n",
    "                      generate_chicago_threat_grid_list, \\\n",
    "                      enrich_with_chicago_grid_200, \\\n",
    "                      CHICAGO_UTM_COORDS, FALSE_LABLE_DATASET_CELL_SIZE, \\\n",
    "                      N_CHICAGO_THREAT_GRID_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_CRIMES_DATA_PATH = os.path.join('data', 'raw', 'Crimes_-_2001_to_present.csv')\n",
    "PROCESSED_CRIMES_DATA_PATH = os.path.join('data', 'processed', 'crime_data.csv')\n",
    "\n",
    "RAW_TWEETS_DATA_WILDCARD_PATH = ('\"' +\n",
    "                                 os.path.join('data', 'raw', 'tweets', '*.json') +\n",
    "                                 '\"')\n",
    "PROCESSED_TWEETS_DATA_PATH = os.path.join('data', 'processed', 'tweets_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./ANLP-Project-Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Time Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-08 ---> 2018-02-19\n"
     ]
    }
   ],
   "source": [
    "print(START_DATE, '--->', END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chicago Crimes Incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PROCESSED_CRIMES_DATA_PATH):\n",
    "    !python3 ./preprocess_crimes_data.py {RAW_CRIMES_DATA_PATH}  {PROCESSED_CRIMES_DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_data = pd.read_csv(PROCESSED_CRIMES_DATA_PATH)\n",
    "crimes_data['timestamp'] = pd.to_datetime(crimes_data['timestamp'], format=CSV_DATE_FORMART).dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10902"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crimes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min   2017-12-08\n",
       "max   2018-02-19\n",
       "Name: timestamp, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crimes_data['timestamp'].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PROCESSED_TWEETS_DATA_PATH):\n",
    "    !python3 -W ignore ./preprocess_tweets_data.py {RAW_TWEETS_DATA_WILDCARD_PATH} {PROCESSED_TWEETS_DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = pd.read_csv(PROCESSED_TWEETS_DATA_PATH)\n",
    "tweets_data['timestamp'] = pd.to_datetime(tweets_data['timestamp'], format=CSV_DATE_FORMART).dt.normalize()\n",
    "tweets_data['tokens'] = tweets_data['tokens'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79634"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min   2017-12-08\n",
       "max   2018-02-19\n",
       "Name: timestamp, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data['timestamp'].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets document groupping by geo-location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tweets_docs(tweets_data):\n",
    "    tweet_docs_groupby = tweets_data.groupby(('latitude_index', 'longitude_index'))\n",
    "    tweet_docs = tweet_docs_groupby['tokens'].apply(lambda r: list(r)).apply(lambda r: sum(r, []))\n",
    "    tweet_docs = tweet_docs.sort_index()\n",
    "    return tweet_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extract Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity\n",
    "\n",
    "def train_KDE_model(train_df, bandwith=KDE_BANDWITH):\n",
    "    '''\n",
    "    Train KDE model.\n",
    "\n",
    "    Input:\n",
    "    train_df: train data frame with Latitude Logitude. 3 months prior data for the day of surveillance..\n",
    "\n",
    "    Output:\n",
    "    KDE Model\n",
    "    '''\n",
    "    \n",
    "    kde = KernelDensity(bandwidth=bandwith,\n",
    "                        metric='haversine',\n",
    "                        kernel='gaussian',\n",
    "                        algorithm='ball_tree')\n",
    "    \n",
    "    kde.fit(train_df[['latitude','longitude']])\n",
    "    \n",
    "    return kde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_kde_model = train_KDE_model(crimes_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO - visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def coalesce(token):\n",
    "    '''\n",
    "    Klaues: why this function?\n",
    "    '''\n",
    "    new_tokens = []\n",
    "    for char in token:\n",
    "        if len(new_tokens) < 2 or char != new_tokens[-1] or char != new_tokens[-2]:\n",
    "            new_tokens.append(char)\n",
    "    return ''.join(new_tokens)\n",
    "\n",
    "def preprocess_tweet_for_LDA(raw_tokens):\n",
    "    '''\n",
    "    text input is one string\n",
    "    output is tokenized and preprocessed(as defined below) text\n",
    "    \n",
    "    lowercase\n",
    "    no hashtags or mentions\n",
    "    any url converted to \"url\"\n",
    "    replace multiple repeated chars with 2 of them. eg paaaarty -> paarty\n",
    "    '''\n",
    "    \n",
    "    processed_tokens = []\n",
    "    for token in raw_tokens:\n",
    "        if token.startswith(\"@\") or token.startswith(\"#\"):\n",
    "            continue\n",
    "        elif token.startswith(\"https://\") or token.startswith(\"http://\"):\n",
    "            processed_tokens.append(\"url\")\n",
    "        else:\n",
    "            processed_tokens.append(coalesce(token))\n",
    "            \n",
    "    return processed_tokens\n",
    "\n",
    "def train_LDA_model(docs, params=LDA_PARAMS, preprocessor=preprocess_tweet_for_LDA):\n",
    "    \n",
    "    vectorizer = CountVectorizer(stop_words=\"english\",\n",
    "                                 preprocessor=preprocessor,\n",
    "                                 tokenizer=lambda x:x)\n",
    "    \n",
    "    lda_train_data = vectorizer.fit_transform(docs)\n",
    "    \n",
    "    lda_model = LatentDirichletAllocation(**params)\n",
    "\n",
    "    lda_model.fit(lda_train_data)\n",
    "    \n",
    "    doc_topics = lda_model.transform(lda_train_data)\n",
    "    \n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "    \n",
    "    return lda_model, doc_topics, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_top_words_LDA(topic_index, lda_model, vocabulary, n_top_words):\n",
    "    topic = lda_model.components_[topic_index]\n",
    "    return [vocabulary[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "            \n",
    "def print_top_words_LDA(lda_model, vocabulary, n_top_words):\n",
    "    for topic_idx in range(len(lda_model.components_)):\n",
    "        \n",
    "        message = \"Topic #%d: \" % topic_index\n",
    "        message += \" | \".join(get_topic_top_words_LDA(topic_index, lda_model, vocabulary, n_top_words))\n",
    "\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "# print_top_words_LDA(tweets_lda_model, tweets_vocabulary, 5)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train & Evaluatuin Datasets Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_step_train_dataset(crimes_dataset, tweets_dataset):\n",
    "    crimes_kde_model = train_KDE_model(crimes_dataset)\n",
    "    \n",
    "    tweets_docs = generate_tweets_docs(tweets_dataset)\n",
    "    \n",
    "    tweets_lda_model, doc_topics, tweets_vocabulary = train_LDA_model(tweets_docs.tolist())\n",
    "    \n",
    "    get_tweets_topic_top_words_LDA = functools.partial(get_topic_top_words_LDA,\n",
    "                                                       lda_model=tweets_lda_model,\n",
    "                                                       vocabulary=tweets_vocabulary,\n",
    "                                                       n_top_words=5)\n",
    "\n",
    "    latlng2LDA_tweet_topics_chicago = functools.partial(latlng2LDA_topics_chicago,\n",
    "                                                    doc_topics=doc_topics,\n",
    "                                                    docs=tweets_docs)\n",
    "\n",
    "    chicago_grid_mask = lambda lat, lng: latlng2grid_cords_chicago(lat, lng) in tweets_docs.index\n",
    "    \n",
    "    train_dataset = pd.concat([crimes_data[['latitude', 'longitude']].assign(crime=True),\n",
    "                            generate_chicago_threat_grid_list().assign(crime=False)],\n",
    "                    axis=0)\n",
    "\n",
    "    '''\n",
    "    train_dataset['exists_tweets_data'] = train_dataset.apply(\n",
    "        lambda r: chicago_grid_mask(r['latitude'], r['longitude']),\n",
    "    axis=1)\n",
    "    train_dataset = train_dataset[train_dataset['exists_tweets_data']]\n",
    "    '''\n",
    "    train_dataset = train_dataset[['latitude', 'longitude', 'crime']]\n",
    "    train_dataset = enrich_with_chicago_grid_200(train_dataset)\n",
    "\n",
    "    train_dataset['KDE'] = crimes_kde_model.score_samples(\n",
    "        train_dataset[['latitude', 'longitude']].as_matrix()\n",
    "    )\n",
    "\n",
    "    train_dataset[LDA_TOPICS] = train_dataset.apply(lambda row: pd.Series(latlng2LDA_tweet_topics_chicago(\n",
    "                                                                    row['latitude'],\n",
    "                                                                    row['longitude'])),\n",
    "                                        axis=1)\n",
    "\n",
    "    features_cols = ['KDE'] + LDA_TOPICS\n",
    "\n",
    "    train_dataset = {\n",
    "                    'X': train_dataset[['latitude_index', 'longitude_index'] + features_cols],\n",
    "                    'Y': train_dataset['crime']\n",
    "    }\n",
    "    \n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def generate_one_step_evaluation_dataset(crimes_evaluation_dataset):\n",
    "    evaluation_dataset = enrich_with_chicago_grid_200(crimes_evaluation_dataset)\n",
    "    evaluation_dataset = evaluation_dataset[['latitude_index', 'longitude_index']]\n",
    "    return evaluation_dataset\n",
    "\n",
    "\n",
    "def generate_one_step_datasets(crimes_data, tweets_data, start_train_date, n_train_days):\n",
    "    start_train_date = pd.to_datetime(start_train_date)\n",
    "\n",
    "    end_train_date = start_train_date + pd.DateOffset(n_train_days)\n",
    "    evaluation_date = end_train_date + pd.DateOffset(1)\n",
    "    \n",
    "    print(start_train_date, '|', end_train_date, '|', evaluation_date)\n",
    "    crimes_train_dataset = crimes_data[(crimes_data['timestamp'] >= start_train_date) &\n",
    "                                       ((crimes_data['timestamp'] <= end_train_date))]\n",
    "    \n",
    "    tweets_train_dataset = tweets_data[(tweets_data['timestamp'] >= start_train_date) &\n",
    "                                       ((tweets_data['timestamp'] <= end_train_date))]\n",
    "    \n",
    "    crimes_evaluation_dataset = crimes_data[crimes_data['timestamp'] == evaluation_date]\n",
    "    \n",
    "    print('Crimes Train Dataset:')\n",
    "    print(crimes_train_dataset['timestamp'].agg([len, 'min', 'max']))\n",
    "      \n",
    "    print('Tweets Train Dataset:')\n",
    "    print(tweets_train_dataset['timestamp'].agg([len, 'min', 'max']))\n",
    "\n",
    "    print('Crimes Evaluation Dataset:')\n",
    "    print(crimes_evaluation_dataset['timestamp'].agg([len, 'min', 'max']))\n",
    "    \n",
    "    train_dataset = generate_one_step_train_dataset(crimes_train_dataset, tweets_train_dataset)\n",
    "    evaluation_dataset = generate_one_step_evaluation_dataset(crimes_evaluation_dataset)\n",
    "    \n",
    "    return train_dataset, evaluation_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def generate_threat_kde_dataset(train_dataset):\n",
    "    threat_grid_cells = train_dataset['X'][~train_dataset['Y']]\n",
    "    kde_values = threat_grid_cells[['latitude_index', 'longitude_index', 'KDE']]\n",
    "    threat_kde_df = kde_values.set_index(['latitude_index', 'longitude_index'])['KDE']\n",
    "    threat_kde_df = threat_kde_df.sort_values(ascending=False)\n",
    "    return list(threat_kde_df.index)\n",
    "\n",
    "def generate_threat_logreg_dataset(train_dataset):\n",
    "    is_crime_count = train_dataset['Y'].value_counts()\n",
    "    logreg_C = is_crime_count[False] / is_crime_count[True]\n",
    "    logreg = LogisticRegression(C=logreg_C)\n",
    "    logreg.fit(train_dataset['X'][['KDE'] + LDA_TOPICS], train_dataset['Y'])\n",
    "    \n",
    "    threat_grid_cells = train_dataset['X'][~train_dataset['Y']]\n",
    "\n",
    "    threat_grid_cells['logreg'] = logreg.predict_log_proba(threat_grid_cells[['KDE'] + LDA_TOPICS])[:, 1]\n",
    "    logreg_values = threat_grid_cells[['latitude_index', 'longitude_index', 'logreg']]\n",
    "\n",
    "    threat_logreg_df = logreg_values.set_index(['latitude_index', 'longitude_index'])['logreg']\n",
    "    threat_logreg_df = threat_logreg_df.sort_values(ascending=False)\n",
    "    \n",
    "    return list(threat_logreg_df.index) \n",
    "\n",
    "def generate_threat_datasets(train_dataset):\n",
    "    \n",
    "    return [generate_threat_kde_dataset(train_dataset),\n",
    "            generate_threat_logreg_dataset(train_dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_one_step_surveillance_data(train_dataset,\n",
    "                                        evaluation_dataset):\n",
    "\n",
    "    print(N_CHICAGO_THREAT_GRID_LIST)\n",
    "    surveillance_data = np.zeros((3, N_CHICAGO_THREAT_GRID_LIST))\n",
    "\n",
    "    threat_datasets = generate_threat_datasets(train_dataset)\n",
    "\n",
    "    crime_counts = evaluation_dataset.groupby(['latitude_index', 'longitude_index']).size()\n",
    "    crime_counts = crime_counts.sort_values(ascending=False)\n",
    "\n",
    "    # real crime occurence is our gold dataset\n",
    "    threat_datasets.append(list(crime_counts.index))\n",
    "    \n",
    "    for threat_model_index, threat_dataset in enumerate(threat_datasets):\n",
    "        print(threat_model_index)\n",
    "        for cell_index, (latitude_index, longitude_index) in enumerate(threat_dataset):\n",
    "            surveillance_data[threat_model_index][cell_index] = crime_counts.get((latitude_index, longitude_index), 0)\n",
    "        \n",
    "    return surveillance_data\n",
    "    \n",
    "    \n",
    "def generate_all_data_surveillance_data(crimes_data, tweets_data, n_train_days):\n",
    "    surveillance_data = np.zeros((3, N_CHICAGO_THREAT_GRID_LIST))\n",
    "\n",
    "    for start_train_date in pd.date_range(START_DATE, END_DATE)[:-(n_train_days+1)]:\n",
    "        print('--->', start_train_date)\n",
    "        \n",
    "        train_dataset, evaluation_dataset = generate_one_step_datasets(crimes_data,\n",
    "                                                                       tweets_data,\n",
    "                                                                       start_train_date,\n",
    "                                                                       n_train_days)\n",
    "\n",
    "        surveillance_data += generate_one_step_surveillance_data(train_dataset,\n",
    "                                                                 evaluation_dataset)\n",
    "\n",
    "        surveillance_data = surveillance_data.cumsum(axis=0)\n",
    "        surveillance_data /= surveillance_data.sum(axis=0)\n",
    "        \n",
    "    return surveillance_data\n",
    "\n",
    "def plot_surveillance_data(surveillance_data):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> 2017-12-08 00:00:00\n",
      "2017-12-08 00:00:00 | 2018-02-06 00:00:00 | 2018-02-07 00:00:00\n",
      "Crimes Train Dataset:\n",
      "len                   9322\n",
      "min    2017-12-08 00:00:00\n",
      "max    2018-02-06 00:00:00\n",
      "Name: timestamp, dtype: object\n",
      "Tweets Train Dataset:\n",
      "len                  73143\n",
      "min    2017-12-08 00:00:00\n",
      "max    2018-02-06 00:00:00\n",
      "Name: timestamp, dtype: object\n",
      "Crimes Evaluation Dataset:\n",
      "len                    116\n",
      "min    2018-02-07 00:00:00\n",
      "max    2018-02-07 00:00:00\n",
      "Name: timestamp, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shlomi/projects/Crime-Prediction-with-Tweets/pipeline/utils/geo.py:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  .astype(int))\n",
      "/Users/shlomi/projects/Crime-Prediction-with-Tweets/pipeline/utils/geo.py:92: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  .astype(int))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/base.py:340: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "/usr/local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1356: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.predict_proba(X))\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "---> 2017-12-09 00:00:00\n",
      "2017-12-09 00:00:00 | 2018-02-07 00:00:00 | 2018-02-08 00:00:00\n",
      "Crimes Train Dataset:\n",
      "len                   9263\n",
      "min    2017-12-09 00:00:00\n",
      "max    2018-02-07 00:00:00\n",
      "Name: timestamp, dtype: object\n",
      "Tweets Train Dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:38: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len                  72239\n",
      "min    2017-12-09 00:00:00\n",
      "max    2018-02-06 00:00:00\n",
      "Name: timestamp, dtype: object\n",
      "Crimes Evaluation Dataset:\n",
      "len                    118\n",
      "min    2018-02-08 00:00:00\n",
      "max    2018-02-08 00:00:00\n",
      "Name: timestamp, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "36575\n",
      "0\n",
      "1\n",
      "2\n",
      "---> 2017-12-10 00:00:00\n",
      "2017-12-10 00:00:00 | 2018-02-08 00:00:00 | 2018-02-09 00:00:00\n",
      "Crimes Train Dataset:\n",
      "len                   9218\n",
      "min    2017-12-10 00:00:00\n",
      "max    2018-02-08 00:00:00\n",
      "Name: timestamp, dtype: object\n",
      "Tweets Train Dataset:\n",
      "len                  68766\n",
      "min    2017-12-10 00:00:00\n",
      "max    2018-02-06 00:00:00\n",
      "Name: timestamp, dtype: object\n",
      "Crimes Evaluation Dataset:\n",
      "len                    106\n",
      "min    2018-02-09 00:00:00\n",
      "max    2018-02-09 00:00:00\n",
      "Name: timestamp, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "36575\n",
      "0\n",
      "1\n",
      "2\n",
      "---> 2017-12-11 00:00:00\n",
      "2017-12-11 00:00:00 | 2018-02-09 00:00:00 | 2018-02-10 00:00:00\n",
      "Crimes Train Dataset:\n",
      "len                   9183\n",
      "min    2017-12-11 00:00:00\n",
      "max    2018-02-09 00:00:00\n",
      "Name: timestamp, dtype: object\n",
      "Tweets Train Dataset:\n",
      "len                  65383\n",
      "min    2017-12-11 00:00:00\n",
      "max    2018-02-06 00:00:00\n",
      "Name: timestamp, dtype: object\n",
      "Crimes Evaluation Dataset:\n",
      "len                    112\n",
      "min    2018-02-10 00:00:00\n",
      "max    2018-02-10 00:00:00\n",
      "Name: timestamp, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "36575\n",
      "0\n",
      "1\n",
      "2\n",
      "---> 2017-12-12 00:00:00\n",
      "2017-12-12 00:00:00 | 2018-02-10 00:00:00 | 2018-02-11 00:00:00\n",
      "Crimes Train Dataset:\n",
      "len                   9105\n",
      "min    2017-12-12 00:00:00\n",
      "max    2018-02-10 00:00:00\n",
      "Name: timestamp, dtype: object\n",
      "Tweets Train Dataset:\n",
      "len                  63137\n",
      "min    2017-12-17 00:00:00\n",
      "max    2018-02-06 00:00:00\n",
      "Name: timestamp, dtype: object\n",
      "Crimes Evaluation Dataset:\n",
      "len                     93\n",
      "min    2018-02-11 00:00:00\n",
      "max    2018-02-11 00:00:00\n",
      "Name: timestamp, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n",
      "36575\n",
      "0\n",
      "1\n",
      "2\n",
      "---> 2017-12-13 00:00:00\n",
      "2017-12-13 00:00:00 | 2018-02-11 00:00:00 | 2018-02-12 00:00:00\n",
      "Crimes Train Dataset:\n",
      "len                   9011\n",
      "min    2017-12-13 00:00:00\n",
      "max    2018-02-11 00:00:00\n",
      "Name: timestamp, dtype: object\n",
      "Tweets Train Dataset:\n",
      "len                  63137\n",
      "min    2017-12-17 00:00:00\n",
      "max    2018-02-06 00:00:00\n",
      "Name: timestamp, dtype: object\n",
      "Crimes Evaluation Dataset:\n",
      "len                    114\n",
      "min    2018-02-12 00:00:00\n",
      "max    2018-02-12 00:00:00\n",
      "Name: timestamp, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n"
     ]
    }
   ],
   "source": [
    "q = generate_all_data_surveillance_data(crimes_data, tweets_data, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(((q.cumsum(axis=1).T / q.sum(axis=1)).T)[0, :])\n",
    "plt.plot(((q.cumsum(axis=1).T / q.sum(axis=1)).T)[1, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
