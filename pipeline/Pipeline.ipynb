{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crime Prediction using Tweets and KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import itertools\n",
    "import functools\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "\n",
    "\n",
    "from utils.consts import START_DATE, END_DATE, KDE_BANDWITH, LDA_PARAMS, LDA_TOPICS, \\\n",
    "                         CSV_DATE_FORMART\n",
    "\n",
    "\n",
    "from utils.geo import latlng2LDA_topics_chicago, latlng2LDA_sentment_chicago, \\\n",
    "                      generate_chicago_threat_grid_list, \\\n",
    "                      enrich_with_chicago_grid_200, \\\n",
    "                      CHICAGO_UTM_COORDS, FALSE_LABLE_DATASET_CELL_SIZE, \\\n",
    "                      N_CHICAGO_THREAT_GRID_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_CRIMES_DATA_PATH = os.path.join('data', 'raw', 'Crimes_-_2001_to_present.csv')\n",
    "PROCESSED_CRIMES_DATA_PATH = os.path.join('data', 'processed', 'crime_data.csv')\n",
    "\n",
    "RAW_TWEETS_DATA_WILDCARD_PATH = ('\"' +\n",
    "                                 os.path.join('data', 'raw', 'tweets', '*.json') +\n",
    "                                 '\"')\n",
    "PROCESSED_TWEETS_DATA_PATH = os.path.join('data', 'processed', 'tweets_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./ANLP-Project-Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Time Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12-08 ---> 2018-02-19\n"
     ]
    }
   ],
   "source": [
    "print(START_DATE, '--->', END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chicago Crimes Incidents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PROCESSED_CRIMES_DATA_PATH):\n",
    "    !python3 ./preprocess_crimes_data.py {RAW_CRIMES_DATA_PATH}  {PROCESSED_CRIMES_DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_data = pd.read_csv(PROCESSED_CRIMES_DATA_PATH)\n",
    "crimes_data['timestamp'] = pd.to_datetime(crimes_data['timestamp'], format=CSV_DATE_FORMART).dt.normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10902"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crimes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min   2017-12-08\n",
       "max   2018-02-19\n",
       "Name: timestamp, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crimes_data['timestamp'].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PROCESSED_TWEETS_DATA_PATH):\n",
    "    !python3 -W ignore ./preprocess_tweets_data.py {RAW_TWEETS_DATA_WILDCARD_PATH} {PROCESSED_TWEETS_DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data = pd.read_csv(PROCESSED_TWEETS_DATA_PATH)\n",
    "tweets_data['timestamp'] = pd.to_datetime(tweets_data['timestamp'], format=CSV_DATE_FORMART).dt.normalize()\n",
    "tweets_data['tokens'] = tweets_data['tokens'].apply(lambda x: eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79634"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min   2017-12-08\n",
       "max   2018-02-19\n",
       "Name: timestamp, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data['timestamp'].agg(['min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extract Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity\n",
    "\n",
    "def train_KDE_model(train_df, bandwith=KDE_BANDWITH):\n",
    "    '''\n",
    "    Train KDE model.\n",
    "\n",
    "    Input:\n",
    "    train_df: train data frame with Latitude Logitude. 3 months prior data for the day of surveillance..\n",
    "\n",
    "    Output:\n",
    "    KDE Model\n",
    "    '''\n",
    "    \n",
    "    kde = KernelDensity(bandwidth=bandwith,\n",
    "                        metric='haversine',\n",
    "                        kernel='gaussian',\n",
    "                        algorithm='ball_tree')\n",
    "    \n",
    "    kde.fit(train_df[['latitude','longitude']])\n",
    "    \n",
    "    return kde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO - visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding ngram features : ngram_range 2\n",
      "Add bigram sentiment scores\n",
      "Add unigram sentiment scores\n"
     ]
    }
   ],
   "source": [
    "from utils.sentiment.sentiment import calculate_sentiment_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_data['sentiment'] = tweets_data['tokens'].apply(lambda x: calculate_sentiment_tweet(' '.join(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def coalesce(token):\n",
    "    '''\n",
    "    Klaues: why this function?\n",
    "    '''\n",
    "    new_tokens = []\n",
    "    for char in token:\n",
    "        if len(new_tokens) < 2 or char != new_tokens[-1] or char != new_tokens[-2]:\n",
    "            new_tokens.append(char)\n",
    "    return ''.join(new_tokens)\n",
    "\n",
    "def preprocess_tweet_for_LDA(raw_tokens):\n",
    "    '''\n",
    "    text input is one string\n",
    "    output is tokenized and preprocessed(as defined below) text\n",
    "    \n",
    "    lowercase\n",
    "    no hashtags or mentions\n",
    "    any url converted to \"url\"\n",
    "    replace multiple repeated chars with 2 of them. eg paaaarty -> paarty\n",
    "    '''\n",
    "    \n",
    "    processed_tokens = []\n",
    "    for token in raw_tokens:\n",
    "        if token.startswith(\"@\") or token.startswith(\"#\"):\n",
    "            continue\n",
    "        elif token.startswith(\"https://\") or token.startswith(\"http://\"):\n",
    "            processed_tokens.append(\"url\")\n",
    "        else:\n",
    "            processed_tokens.append(coalesce(token))\n",
    "            \n",
    "    return processed_tokens\n",
    "\n",
    "def train_LDA_model(docs, params=LDA_PARAMS, preprocessor=preprocess_tweet_for_LDA):\n",
    "    \n",
    "    vectorizer = CountVectorizer(stop_words=\"english\",\n",
    "                                 preprocessor=preprocessor,\n",
    "                                 tokenizer=lambda x:x)\n",
    "    \n",
    "    lda_train_data = vectorizer.fit_transform(docs)\n",
    "    \n",
    "    lda_model = LatentDirichletAllocation(**params)\n",
    "\n",
    "    lda_model.fit(lda_train_data)\n",
    "    \n",
    "    doc_topics = lda_model.transform(lda_train_data)\n",
    "    \n",
    "    vocabulary = vectorizer.get_feature_names()\n",
    "    \n",
    "    return lda_model, doc_topics, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_top_words_LDA(topic_index, lda_model, vocabulary, n_top_words):\n",
    "    topic = lda_model.components_[topic_index]\n",
    "    return [vocabulary[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "            \n",
    "def print_top_words_LDA(lda_model, vocabulary, n_top_words):\n",
    "    for topic_index in range(len(lda_model.components_)):\n",
    "        \n",
    "        message = \"Topic #%d: \" % topic_index\n",
    "        message += \" | \".join(get_topic_top_words_LDA(topic_index, lda_model, vocabulary, n_top_words))\n",
    "\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "# print_top_words_LDA(tweets_lda_model, tweets_vocabulary, 5)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Train & Evaluatuin Datasets Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tweets_docs(tweets_data):\n",
    "    tweet_docs_groupby = tweets_data.groupby(('latitude_index', 'longitude_index'))\n",
    "    tweet_docs = tweet_docs_groupby['tokens'].apply(lambda r: list(r))\n",
    "    tweet_docs = tweet_docs.sort_index()\n",
    "    return tweet_docs, tweet_docs_groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_by_date_window(dataset, start_end, end_date):\n",
    "        return dataset[(dataset['timestamp'] >= start_end) &\n",
    "                        ((dataset['timestamp'] <= end_date))]\n",
    "    \n",
    "\n",
    "def build_datasets_by_date_window(crimes_data, tweets_data, start_train_date, n_train_days):\n",
    "\n",
    "    start_train_date = pd.to_datetime(start_train_date)\n",
    "    end_train_date = start_train_date + pd.DateOffset(n_train_days)\n",
    "    evaluation_date = end_train_date + pd.DateOffset(1)\n",
    "\n",
    "    crimes_train_dataset = filter_dataset_by_date_window(crimes_data, start_train_date, end_train_date)\n",
    "    tweets_train_dataset = filter_dataset_by_date_window(tweets_data, start_train_date, end_train_date)\n",
    "    crimes_evaluation_dataset = filter_dataset_by_date_window(crimes_data, evaluation_date, evaluation_date)\n",
    "\n",
    "    return crimes_train_dataset, tweets_train_dataset, crimes_evaluation_dataset\n",
    "\n",
    "\n",
    "def generate_one_step_train_dataset(crimes_dataset, tweets_dataset):\n",
    "    crimes_kde_model = train_KDE_model(crimes_dataset)\n",
    "    \n",
    "    tweets_docs, tweet_docs_groupby = generate_tweets_docs(tweets_dataset)\n",
    "    \n",
    "    average_sentiment_docs = tweet_docs_groupby['sentiment'].agg('mean')\n",
    "    \n",
    "    latlng2LDA_tweet_sentiment_chicago = functools.partial(latlng2LDA_sentment_chicago,\n",
    "                                                average_sentiment_docs=average_sentiment_docs)\n",
    "    \n",
    "    tweets_lda_model, doc_topics, tweets_vocabulary = train_LDA_model((tweets_docs\n",
    "                                                                       .apply(lambda r: sum(r, []))\n",
    "                                                                       .tolist()))\n",
    "    '''\n",
    "    get_tweets_topic_top_words_LDA = functools.partial(get_topic_top_words_LDA,\n",
    "                                                       lda_model=tweets_lda_model,\n",
    "                                                       vocabulary=tweets_vocabulary,\n",
    "                                                       n_top_words=5)\n",
    "    '''\n",
    "    \n",
    "    latlng2LDA_tweet_topics_chicago = functools.partial(latlng2LDA_topics_chicago,\n",
    "                                                    doc_topics=doc_topics,\n",
    "                                                    docs=tweets_docs)\n",
    "    \n",
    "    train_dataset = pd.concat([enrich_with_chicago_grid_200(crimes_data[['latitude', 'longitude']]).assign(crime=True),\n",
    "                            generate_chicago_threat_grid_list().assign(crime=False)],\n",
    "                    axis=0)\n",
    "\n",
    "\n",
    "    train_dataset = train_dataset[['latitude', 'longitude', 'latitude_index', 'longitude_index', 'crime']]\n",
    "\n",
    "    train_dataset['KDE'] = crimes_kde_model.score_samples(\n",
    "        train_dataset[['latitude', 'longitude']].as_matrix()\n",
    "    )\n",
    "\n",
    "    train_dataset['SENTIMENT'] = train_dataset.apply(lambda row: pd.Series(latlng2LDA_tweet_sentiment_chicago(\n",
    "                                                                    row['latitude'],\n",
    "                                                                    row['longitude'])),\n",
    "                                        axis=1)\n",
    "\n",
    "    \n",
    "    train_dataset[LDA_TOPICS] = train_dataset.apply(lambda row: pd.Series(latlng2LDA_tweet_topics_chicago(\n",
    "                                                                    row['latitude'],\n",
    "                                                                    row['longitude'])),\n",
    "                                        axis=1)\n",
    "\n",
    "    features_cols = ['KDE', 'SENTIMENT'] + LDA_TOPICS\n",
    "\n",
    "    train_dataset = {\n",
    "                    'X'         : train_dataset[['latitude_index', 'longitude_index'] + features_cols],\n",
    "                    'Y'         : train_dataset['crime'],\n",
    "                    'KDE'       : crimes_kde_model,\n",
    "                    'SENTIMENT' : average_sentiment_docs,\n",
    "                    'LDA'       : {'model': tweets_lda_model,'vocabulary': tweets_vocabulary}\n",
    "    }\n",
    "    \n",
    "    return train_dataset\n",
    "\n",
    "\n",
    "def generate_one_step_evaluation_dataset(crimes_evaluation_dataset):\n",
    "    evaluation_dataset = enrich_with_chicago_grid_200(crimes_evaluation_dataset)\n",
    "    evaluation_dataset = evaluation_dataset[['latitude_index', 'longitude_index']]\n",
    "    return evaluation_dataset\n",
    "\n",
    "\n",
    "def generate_one_step_datasets(crimes_data, tweets_data, start_train_date, n_train_days):\n",
    "    \n",
    "    crimes_train_dataset, tweets_train_dataset, crimes_evaluation_dataset = build_datasets_by_date_window(crimes_data,\n",
    "                                                                                                          tweets_data,\n",
    "                                                                                                          start_train_date,\n",
    "                                                                                                          n_train_days)\n",
    "    train_dataset = generate_one_step_train_dataset(crimes_train_dataset, tweets_train_dataset)\n",
    "    evaluation_dataset = generate_one_step_evaluation_dataset(crimes_evaluation_dataset)\n",
    "    \n",
    "    return train_dataset, evaluation_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def generate_threat_kde_dataset(train_dataset):\n",
    "    threat_grid_cells = train_dataset['X'][~train_dataset['Y']]\n",
    "    kde_values = threat_grid_cells[['latitude_index', 'longitude_index', 'KDE']]\n",
    "    \n",
    "    threat_kde_df = kde_values.set_index(['latitude_index', 'longitude_index'])['KDE']\n",
    "    threat_kde_df = threat_kde_df.sort_values(ascending=False)\n",
    "    \n",
    "    return list(threat_kde_df.index)\n",
    "\n",
    "def generate_threat_logreg_dataset(train_dataset):\n",
    "    is_crime_count = train_dataset['Y'].value_counts()\n",
    "    logreg_C = is_crime_count[False] / is_crime_count[True]\n",
    "    logreg = LogisticRegression(C=logreg_C)\n",
    "    logreg.fit(train_dataset['X'][['KDE', 'sentiment'] + LDA_TOPICS], train_dataset['Y'])\n",
    "    \n",
    "    threat_grid_cells = train_dataset['X'][~train_dataset['Y']]\n",
    "    threat_grid_cells['logreg'] = logreg.predict_log_proba(threat_grid_cells[['KDE'] + LDA_TOPICS])[:, 1]\n",
    "    \n",
    "    logreg_values = threat_grid_cells[['latitude_index', 'longitude_index', 'logreg']]\n",
    "    threat_logreg_df = logreg_values.set_index(['latitude_index', 'longitude_index'])['logreg']\n",
    "    threat_logreg_df = threat_logreg_df.sort_values(ascending=False)\n",
    "    \n",
    "    return list(threat_logreg_df.index) \n",
    "\n",
    "def generate_threat_datasets(train_dataset):\n",
    "    \n",
    "    return [generate_threat_kde_dataset(train_dataset),\n",
    "            generate_threat_logreg_dataset(train_dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_surveillance_data(train_dataset, evaluation_dataset):\n",
    "\n",
    "    surveillance_data = np.zeros((3, N_CHICAGO_THREAT_GRID_LIST))\n",
    "\n",
    "    threat_datasets = generate_threat_datasets(train_dataset)\n",
    "\n",
    "    crime_counts = evaluation_dataset.groupby(['latitude_index', 'longitude_index']).size()\n",
    "    crime_counts = crime_counts.sort_values(ascending=False)\n",
    "\n",
    "    # real crime occurence is our gold dataset\n",
    "    threat_datasets.append(list(crime_counts.index))\n",
    "    \n",
    "    for threat_model_index, threat_dataset in enumerate(threat_datasets):\n",
    "        for cell_index, (latitude_index, longitude_index) in enumerate(threat_dataset):\n",
    "            surveillance_data[threat_model_index][cell_index] = crime_counts.get((latitude_index, longitude_index), 0)\n",
    "        \n",
    "    return surveillance_data\n",
    "    \n",
    "    \n",
    "def generate_one_step_surveillance_data(crimes_data, tweets_data, start_train_date, n_train_days):\n",
    "    \n",
    "        train_dataset, evaluation_dataset = generate_one_step_datasets(crimes_data,\n",
    "                                                                       tweets_data,\n",
    "                                                                       start_train_date,\n",
    "                                                                       n_train_days)\n",
    "\n",
    "        surveillance_data = generate_surveillance_data(train_dataset,\n",
    "                                                        evaluation_dataset)\n",
    "        \n",
    "        return surveillance_data\n",
    "\n",
    "    \n",
    "def generate_all_data_surveillance_data(crimes_data, tweets_data, n_train_days):\n",
    "    surveillance_data = []#np.zeros((3, N_CHICAGO_THREAT_GRID_LIST))\n",
    "\n",
    "    for start_train_date in tqdm(pd.date_range(START_DATE, END_DATE)[:-(n_train_days+1)][:3]):\n",
    "        \n",
    "        surveillance_data.append((start_train_date,  generate_one_step_surveillance_data(crimes_data,\n",
    "                                                                  tweets_data,\n",
    "                                                                  start_train_date,\n",
    "                                                                  n_train_days)))\n",
    "\n",
    "\n",
    "#        surveillance_data = surveillance_data.cumsum(axis=0)\n",
    "#        surveillance_data /= surveillance_data.sum(axis=0)\n",
    "        \n",
    "    return surveillance_data\n",
    "\n",
    "def plot_surveillance_data(surveillance_data):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pandas/core/indexing.py:357: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/usr/local/lib/python3.6/site-packages/pandas/core/indexing.py:621: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Wrong number of items passed 42, placement implies 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2521\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2523\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SENTIMENT'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value, check)\u001b[0m\n\u001b[1;32m   3962\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3963\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3964\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2523\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2524\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SENTIMENT'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-7fb16729b247>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_one_step_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrimes_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTART_DATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-8948eeea9554>\u001b[0m in \u001b[0;36mgenerate_one_step_datasets\u001b[0;34m(crimes_data, tweets_data, start_train_date, n_train_days)\u001b[0m\n\u001b[1;32m     88\u001b[0m                                                                                                           \u001b[0mstart_train_date\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                                                                                                           n_train_days)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_one_step_train_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrimes_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweets_train_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mevaluation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_one_step_evaluation_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrimes_evaluation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-8948eeea9554>\u001b[0m in \u001b[0;36mgenerate_one_step_train_dataset\u001b[0;34m(crimes_dataset, tweets_dataset)\u001b[0m\n\u001b[1;32m     55\u001b[0m                                                                     \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'latitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                                                                     row['longitude'])),\n\u001b[0;32m---> 57\u001b[0;31m                                         axis=1)\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2518\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2519\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2584\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2586\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2588\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1954\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1955\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value, check)\u001b[0m\n\u001b[1;32m   3964\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3965\u001b[0m             \u001b[0;31m# This item wasn't present, just insert at end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3967\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3968\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, item, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   4065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4066\u001b[0m         block = make_block(values=value, ndim=self.ndim,\n\u001b[0;32m-> 4067\u001b[0;31m                            placement=slice(loc, loc + 1))\n\u001b[0m\u001b[1;32m   4068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4069\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mblkno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_fast_count_smallints\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blknos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   2950\u001b[0m                      placement=placement, dtype=dtype)\n\u001b[1;32m   2951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2952\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfastpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m \u001b[0;31m# TODO: flexible with index=None and/or items=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim, fastpath)\u001b[0m\n\u001b[1;32m    118\u001b[0m             raise ValueError('Wrong number of items passed %d, placement '\n\u001b[1;32m    119\u001b[0m                              'implies %d' % (len(self.values),\n\u001b[0;32m--> 120\u001b[0;31m                                              len(self.mgr_locs)))\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 42, placement implies 1"
     ]
    }
   ],
   "source": [
    "train_dataset, evaluation_dataset = generate_one_step_datasets(crimes_data, tweets_data, START_DATE, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_params = LDA_PARAMS.copy()\n",
    "lda_params['n_components'] = 500\n",
    "tweets_lda_model, doc_topics, tweets_vocabulary = train_LDA_model((tweets_docs\n",
    "                                                                   .apply(lambda r: sum(r, []))\n",
    "                                                                   .tolist()),\n",
    "                                                                 params=lda_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = [tweets_lda_model.components_[i]\n",
    "      for i in range(LDA_PARAMS['n_components'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_lda_model.components_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(set(x)) for x in tt].count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words_LDA(tweets_lda_model, tweets_vocabulary, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words_LDA(train_dataset['LDA']['model'], train_dataset['LDA']['vocabulary'], 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimes_data['longitude_index'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_crime_count = train_dataset['Y'].value_counts()\n",
    "logreg_C = is_crime_count[False] / is_crime_count[True]\n",
    "logreg = LogisticRegression(C=logreg_C)\n",
    "logreg.fit(train_dataset['X'][['KDE'] + LDA_TOPICS], train_dataset['Y'])\n",
    "\n",
    "threat_grid_cells = train_dataset['X'][~train_dataset['Y']]\n",
    "threat_grid_cells['logreg'] = logreg.predict_log_proba(threat_grid_cells[['KDE'] + LDA_TOPICS])[:, 1]\n",
    "\n",
    "logreg_values = threat_grid_cells[['latitude_index', 'longitude_index', 'logreg']]\n",
    "threat_logreg_df = logreg_values.set_index(['latitude_index', 'longitude_index'])['logreg']\n",
    "threat_logreg_df = threat_logreg_df.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(((logreg.coef_[0]/logreg.coef_[0][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(logreg.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.coef_[0]XXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = q\n",
    "q = w[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ww = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ww[0]-ww[1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = int(q.shape[1] / 100)\n",
    "qq = (q.cumsum(axis=1) / q.sum(axis=1)[:,None])[:,::step]\n",
    "\n",
    "plt.plot(qq[0])\n",
    "plt.plot(qq[1][::step])\n",
    "#plt.plot(qq[2])\n",
    "plt#.plot(qq[1] - qq[0])\n",
    "plt.xticks(range(0, 120, 20))\n",
    "plt.yticks(np.arange(0, 1.2, 0.2), range(0, 120, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qq[2,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
